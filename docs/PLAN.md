# Implementation Plan: Learning Raft in Rust

> This plan breaks the project down into **incremental, test-driven work units** that build on one another.  Each task specifies the *artifact(s) produced*, the *appropriate testing strategy*, and (where we consciously trade depth for simplicity) a **ğŸš§ Future Depth** note that sketches potential next steps.

---

## Legend
| Phase | Scope |
|-------|-------|
| ğŸ› ï¸  Work Unit | Concrete implementation task |
| âœ… Test | Unit, Integration (IT), Deterministic Cluster (DC), End-to-End (E2E) |
| ğŸš§ Future Depth | Possible advanced extension |

---

## Phase 0 â€“ Repository Skeleton & CI
1. ğŸ› ï¸  **Create Cargo workspace structure** (`raft-core`, `raft-transport`, `raft-storage`, `examples`).
   * âœ… *Unit*: `cargo check`, `cargo test` placeholder.
2. ğŸ› ï¸  **Set up Continuous Integration** (GitHub Actions): `fmt`, `clippy`, `test` matrix.
   * âœ… *IT*: workflow executes on PRs.
   * ğŸš§ **Future Depth**: add MIRI, loom or shuttle for concurrency testing.

---

## Phase 1 â€“ Foundations
### 1.1 Core Types & Proto
* ğŸ› ï¸  Define shared types (`NodeId`, `Term`, `Index`, `LogEntry`, errors).
* ğŸ› ï¸  Generate Rust code from `proto/raft/v1/raft.proto` with `prost` + `tonic`.
  * âœ… *Unit*: (de)serialization round-trip tests for all protobuf messages.
  * ğŸš§ **Future Depth**: enable Protobuf compatibility tests across language bindings.

### 1.2 Trait Contracts
* ğŸ› ï¸  Draft **`Storage`** and **`Transport`** traits (see overview).
  * âœ… *Unit*: compile-time trait bounds, basic mock impls.
  * ğŸš§ Provide generic `StateMachine` trait for user commands.

---

## Phase 2 â€“ In-Memory Implementations (Facilitate early testing)
### 2.1 In-Memory Storage
* ğŸ› ï¸  Simple `Vec<LogEntry>` + `HashMap` persistence in memory.
  * âœ… *Unit*: append/read semantics, term & index invariants.
  * ğŸš§ **Future Depth**: durability guarantees via disk WAL (see Phase 4).

### 2.2 Mock Transport
* ğŸ› ï¸  `ChannelTransport` using Tokio channels to simulate an in-process network.
  * âœ… *IT*: message ordering, loss, duplication scenarios.
  * ğŸš§ Introduce pluggable network partitions/fault injection for DC tests.

---

## Phase 3 â€“ Raft Core: Leader Election
1. ğŸ› ï¸  Implement **Follower â†’ Candidate â†’ Leader** state transitions, election timers, vote RPC handling.
   * âœ… *Unit*: pure state machine transition tests with mocked storage.
   * âœ… *IT*: 3-node cluster with mock transport reaches single leader.
   * âœ… *DC*: deterministic test harness controlling timers to ensure reproducibility.
   * ğŸš§ **Future Depth**: *pre-vote*, *leadership transfer*, optimized randomised timeouts.

---

## Phase 4 â€“ Log Replication & Commit
1. ğŸ› ï¸  AppendEntries request/response logic on leader & follower.
2. ğŸ› ï¸  Leader commit index advancement & follower catch-up.
   * âœ… *Unit*: log matching property, commit rules.
   * âœ… *IT*: leader replicates commands to followers under normal operation.
   * âœ… *DC*: simulate follower lag, leader crash, new election recovery.
   * ğŸš§ Snapshotting & log compaction (planned Phase 7).

---

## Phase 5 â€“ Disk-Backed Write-Ahead Log (WAL)
1. ğŸ› ï¸  Implement minimal append-only WAL using Tokio FS (`raft-storage/wal.rs`).
2. ğŸ› ï¸  Crash-recovery: reload HardState + log on startup.
   * âœ… *Unit*: fsync ordering, checksum validation.
   * âœ… *IT*: property tests with simulated crashes (close & reopen).
   * ğŸš§ **Future Depth**: segmented log files, mmap, sync batching.

---

## Phase 6 â€“ gRPC Transport
1. ğŸ› ï¸  `GrpcTransport` wrapping `tonic::client::Grpc` stubs.
2. ğŸ› ï¸  `RaftService` server implementation delegating to `RaftNode`.
   * âœ… *IT*: loopback networking; ensure protobuf compatibility.
   * âœ… *E2E*: spin up N binaries in subprocesses communicating via TCP.
   * ğŸš§ TLS, authentication, connection pooling.

---

## Phase 7 â€“ Snapshotting (Optional Stretch)
* ğŸ› ï¸  InstallSnapshot RPC handling, snapshot file format, install path.
  * âœ… *IT*: follower too far behind receives snapshot.
  * ğŸš§ Incremental/streaming snapshots, install chunking.

---

## Phase 8 â€“ Observability & Tooling
1. ğŸ› ï¸  Integrate `tracing` with structured spans (election, replication, IO).
2. ğŸ› ï¸  Provide `examples/cli.rs` for spawning local clusters and issuing commands.
   * âœ… *E2E*: demonstration script verifying replicated counter.
   * ğŸš§ Prometheus metrics, Jaeger tracing, Grafana dashboards.

---

## Phase 9 â€“ Comprehensive Test Matrix & Documentation
1. ğŸ› ï¸  Consolidate tests into `tests/` with cargo-nextest or similar.
2. ğŸ› ï¸  Document expected behaviour, runbooks, contribution guide.

---

## Timeline (Indicative)
| Week | Focus |
|------|-------|
| 1 | Phase 0-1 |
| 2 | Phase 2 |
| 3-4 | Phase 3 |
| 5-6 | Phase 4 |
| 7 | Phase 5 |
| 8 | Phase 6 |
| 9 | Phase 7-8 |
| 10 | Phase 9, buffer & polish |

> **Reminder:** Scope can be adapted; correctness & learnings trump calendar precision.

---

## Trade-Off Summary
Throughout the plan we favour **simplicity & correctness** over completeness. Areas marked with ğŸš§ intentionally defer complexity until the core algorithm is solid. These markers serve as breadcrumbs for future deep dives once the foundational learning goals are achieved.
